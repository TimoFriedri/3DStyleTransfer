{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LICENSE: GPL 3.0\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\\\n",
    "Voxel-based Neural Style Transfer\\\n",
    "Copyright (c)\n",
    "\n",
    "Honda Research Institute Europe GmbH\\\n",
    "Authors: Timo Friedrich\\\n",
    "Contact: timo.friedrich@honda-ri.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel-based Neural Style Transfer\n",
    "We provide the code to reproduce our voxel-based Neural Style Transfer results from the following publications.\n",
    "- Friedrich, T., Wollstadt, P., & Menzel, S. (2020). The Effects of Non-linear Operators in Voxel-Based Deep Neural Networks for 3D Style Reconstruction. 2020 IEEE Symposium Series on Computational Intelligence (SSCI), 1460–1468. https://doi.org/10.1109/SSCI47803.2020.9308308\n",
    "\n",
    "- Friedrich, T., Hammer, B., & Menzel, S. (2021). Voxel-Based Three-Dimensional Neural Style Transfer. 16th International Work-Conference on Artificial Neural Networks, IWANN 2021, 334–346. https://doi.org/10.1007/978-3-030-85030-2_28\n",
    "\n",
    "The classification network Net_belu from our papers is included as well as some programmatically created voxel models. We also provide a voxelized Stanford bunny with a resolution of 256 voxel per dimension. \n",
    "http://graphics.stanford.edu/data/3Dscanrep/\n",
    "\n",
    "---\n",
    "## Environment\n",
    "Create a compatible Conda Environment:\n",
    "\n",
    "```\n",
    "conda create --name EigenEnv --file requirements.txt\n",
    "conda activate EigenEnv\n",
    "```\n",
    "\n",
    "You need a machine with sufficient GPU memory. We used a nVidia Quadro RTX8000 with >40GB memory.\n",
    "\n",
    "---\n",
    "## Additional Data\n",
    "In case you like to try other 3D objects, we refer to the [ModelNet40](https://modelnet.cs.princeton.edu/) dataset.\n",
    "Please use the following commands to voxelize the mesh-based models.\n",
    "[Binvox voxelization tool](https://www.patrickmin.com/binvox/)\n",
    "\n",
    "Command: \n",
    "Either use:\n",
    "```\n",
    "binvox -aw -dc -cb -pb -down -d 448 modelnet40object.off\n",
    "```\n",
    "and manually add a 16 voxel padding programmatically before applying style transfer (recommended) or use:\n",
    "```\n",
    "binvox -aw -dc -cb -pb -down -d 512 modelnet40object.off\n",
    "```\n",
    "but consider that the actual voxel objects extends now to the maximum voxel expansion which has negativ effects during Neural Style Transfer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8ajP_u73s6m"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqxUicSPUOP6"
   },
   "source": [
    "### Import and configure modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sc1OLbOWhPCO"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from time import sleep\n",
    "import functools\n",
    "import shutil\n",
    "import pickle\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "import os\n",
    "os.putenv('AUTOGRAPH_VERBOSITY', '10')\n",
    "#from geometric_learning.voxel.helpers import *\n",
    "import pymrt as pm # version 0.0.2.5!\n",
    "import pymrt.geometry\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "import yaml\n",
    "import binvox_rw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Voxel models\n",
    "The following function provides compatible voxel models by a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voxel(name):\n",
    "    ''' \n",
    "    Returns voxel model in TensorFlow input layer format\n",
    "    \n",
    "    name (str): name of the to be received voxel model \n",
    "    '''\n",
    "    r=256\n",
    "    \n",
    "    if name == 'sphere81':\n",
    "        v = pm.geometry.sphere((r,r,r), 81 , 0.5).astype(np.float32) # sphere, similar volume as bunny\n",
    "        return v[np.newaxis,:,:,:,np.newaxis] \n",
    "    \n",
    "    elif name == 'cube131':\n",
    "        v = pm.geometry.cube((r,r,r), 131 , 0.5).astype(np.float32) #cube, , similar volume as bunny\n",
    "        return v[np.newaxis,:,:,:,np.newaxis] \n",
    "    \n",
    "    elif name == 'cubic6028':\n",
    "        # target mass = bunny\n",
    "        cb = np.zeros([256,256,256], dtype=np.float32)\n",
    "        n = 60 # half edge ength\n",
    "        cb[128-n:128+n, 128-n:128+n, 128-n:128+n] = 1\n",
    "        m = 28\n",
    "        cb[128+n:128+n+m, 128-m:128+m,    128-m:128+m] = 1\n",
    "        cb[128-m:128+m,   128+n:128+n+m,  128-m:128+m] = 1\n",
    "        cb[128-m:128+m,   128-m:128+m,    128+n:128+n+m] = 1\n",
    "        cb[128-n-m:128-n, 128-m:128+m,   128-m:128+m] = 1\n",
    "        cb[128-m:128+m,   128-n-m:128-n, 128-m:128+m] = 1\n",
    "        cb[128-m:128+m,   128-m:128+m,   128-n-m:128-n] = 1\n",
    "        return cb[np.newaxis,:,:,:,np.newaxis]    \n",
    "  \n",
    "    elif name == 'bunny224':\n",
    "        with open('./data/stanford_bunny_224.binvox', 'rb') as f:\n",
    "            vj = binvox_rw.read_as_3d_array(f)\n",
    "            vj = np.pad(vj.data, pad_width=16, mode='constant', constant_values=0)\n",
    "        return vj[np.newaxis,:,:,:,np.newaxis].astype(np.float32)\n",
    "           \n",
    "    else:\n",
    "        raise ValueError(f'{name} not known yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ST Code Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "Jt3i3RRrJiOX"
   },
   "outputs": [],
   "source": [
    "# A Keras model that returns the style and content tensors for the given layer of the provided neural network.\n",
    "class ResponseExtractor(tf.keras.models.Model):\n",
    "    def __init__(self, vnet_file, style_layer_names, content_layer_names ):\n",
    "        \"\"\"\n",
    "        vnet_file             (str):       Path of a saved Keras classification network model\n",
    "        style_layer_names     (list(str)): Layer names for style map extraction\n",
    "        content_layer_names:  (list(str)): Layer names for content map extraction\n",
    "        \"\"\"\n",
    "        super(ResponseExtractor, self).__init__()\n",
    "        \n",
    "        self.vnet = tf.keras.models.load_model(vnet_file)\n",
    "        self.vnet.trainable = False\n",
    "        \n",
    "        layer_names = style_layer_names + content_layer_names\n",
    "        output_layers = [self.vnet.get_layer(name).output for name in layer_names]\n",
    "        self.vnetLayerResponse =  tf.keras.Model([self.vnet.input], output_layers)\n",
    "        self.vnetLayerResponse.trainable = False\n",
    "        \n",
    "        self.style_layer_names = style_layer_names\n",
    "        self.content_layer_names = content_layer_names\n",
    "        self.num_style_layers = len(style_layer_names)\n",
    "\n",
    "\n",
    "    def call(self, voxel):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        \n",
    "        layer_responses = self.vnetLayerResponse(voxel)\n",
    "        style_layer_responses, content_layer_responses = (layer_responses[:self.num_style_layers], \n",
    "                                                        layer_responses[self.num_style_layers:])\n",
    "        return style_layer_responses, content_layer_responses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "Jt3i3RRrJiOX"
   },
   "outputs": [],
   "source": [
    "# Keras Model which calculates the Style Transfer loss value including the regularization terms.\n",
    "# The init function does precomputation if necessary to speed up the optimization loop\n",
    "class LossHandler(tf.keras.models.Model):\n",
    "    def __init__(self, \n",
    "                 map_extractor,\n",
    "                 style_voxel,\n",
    "                 content_voxel,\n",
    "                 content_loss_type,\n",
    "                 style_loss_type, \n",
    "                 num_content_layers, \n",
    "                 num_style_layers,\n",
    "                 style_weight,\n",
    "                 style_layer_weights,\n",
    "                 content_weight,\n",
    "                 total_variation_weight,\n",
    "                 dist_reg_weight,\n",
    "                 dist_reg_padding,\n",
    "                 sym_reg_weight,\n",
    "                 bor_reg_weight,\n",
    "                 bor_reg_margin,\n",
    "                ):\n",
    "        super(LossHandler, self).__init__()   \n",
    "\n",
    "        self.response_extractor = response_extractor\n",
    "        self.style_voxel = style_voxel\n",
    "        self.content_voxel = content_voxel\n",
    "        self.content_loss_type = content_loss_type \n",
    "        self.style_loss_type = style_loss_type\n",
    "        self.num_content_layers = num_content_layers\n",
    "        self.num_style_layers = num_style_layers\n",
    "        self.style_weight = style_weight\n",
    "        self.style_layer_weights = style_layer_weights\n",
    "        self.content_weight = content_weight\n",
    "        self.total_variation_weight = total_variation_weight\n",
    "        self.dist_reg_weight = dist_reg_weight\n",
    "        self.dist_reg_padding = dist_reg_padding\n",
    "        self.sym_reg_weight = sym_reg_weight \n",
    "        self.bor_reg_weight = bor_reg_weight\n",
    "        self.bor_reg_margin = bor_reg_margin\n",
    "        \n",
    "        self.dist_matrix = None\n",
    "        self.borreg_matrix = None  \n",
    "        \n",
    "        # setup loss functions\n",
    "        if self.content_loss_type == 'gatys':\n",
    "            self.content_loss_fcn = self.content_loss_gatys\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "            \n",
    "        if self.style_loss_type == 'hri_std':\n",
    "            self.style_loss_fcn = self.style_loss_gatys\n",
    "            self.style_gram_fcn = self.gram_matrix_hri_standardized\n",
    "        elif self.style_loss_type == 'gatys':\n",
    "            self.style_loss_fcn = self.style_loss_gatys\n",
    "            self.style_gram_fcn = self.gram_matrix_gatys\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "            \n",
    "        ### Precalculations    \n",
    "        \n",
    "        # get the constant style and content values of the targets\n",
    "        self.style_responses, _  = self.response_extractor(self.style_voxel)\n",
    "        _, self.content_responses  = self.response_extractor(self.content_voxel)\n",
    "        # calc constant grams for the style target shape    \n",
    "        self.style_grams = [self.style_gram_fcn(r) for r in self.style_responses]\n",
    "                \n",
    "        # distant material to content regularization\n",
    "        if self.dist_reg_weight > 0:\n",
    "            print('Caluclating distance matrix for the given content shape. This can take some minutes')\n",
    "            # precalc the distance matrix\n",
    "            dist_matrix = get_distance_matrix(self.content_voxel.numpy()[0,:,:,:,0].squeeze(), padding=self.dist_reg_padding)\n",
    "    #         dist_matrix = dist_matrix / np.max(dist_matrix)\n",
    "            self.dist_matrix = tf.Variable(dist_matrix[np.newaxis,:,:,:,np.newaxis].astype(np.float32))\n",
    "    \n",
    "        # border regulariztion matrix\n",
    "        if self.bor_reg_weight > 0:\n",
    "            self.borreg_matrix = get_borreg_matrix(self.content_voxel.numpy().shape, self.bor_reg_margin)\n",
    "    \n",
    "    def call(self, itshape_voxel):\n",
    "        \"\"\"call to get losses for the current iterated shape\"\"\"\n",
    "        itshape_style_responses, itshape_content_responses = self.response_extractor(itshape_voxel)\n",
    "        \n",
    "        # TODO choose which variant here\n",
    "        content_loss = self.content_loss_fcn(itshape_content_responses)\n",
    "        style_loss = self.style_loss_fcn(itshape_style_responses)\n",
    "        \n",
    "        # Regularization terms\n",
    "        loss_totalvar = 0\n",
    "        if self.total_variation_weight > 0:\n",
    "            loss_totalvar = self.total_variation_weight * total_variation_loss(voxel)          \n",
    "\n",
    "        loss_distreg = 0\n",
    "        if self.dist_reg_weight > 0:\n",
    "            loss_distreg = tf.math.abs(itshape_voxel - self.content_voxel)\n",
    "            loss_distreg = tf.math.multiply(loss_distreg, self.dist_matrix) \n",
    "            loss_distreg = tf.math.reduce_sum(loss_distreg) * self.dist_reg_weight\n",
    "\n",
    "        loss_symreg = 0\n",
    "        if self.sym_reg_weight > 0:\n",
    "            loss_symreg = self.sym_reg_weight * symmetry_loss(itshape_voxel)\n",
    "\n",
    "        loss_borreg = 0\n",
    "        if self.bor_reg_weight > 0:\n",
    "            loss_borreg = tf.math.reduce_sum(tf.math.multiply(self.borreg_matrix, itshape_voxel))\n",
    "            loss_borreg = self.bor_reg_weight * loss_borreg\n",
    "            \n",
    "        total =  style_loss + content_loss + loss_totalvar + loss_distreg + loss_symreg + loss_borreg\n",
    "        \n",
    "        return {'total': total,\n",
    "                'style': style_loss,\n",
    "                'content': content_loss,\n",
    "                'reg_tvar': loss_totalvar,\n",
    "                'reg_dist': loss_distreg,\n",
    "                'reg_sym': loss_symreg,\n",
    "                'reg_bor': loss_borreg,\n",
    "               }\n",
    "    \n",
    "    # gram matrix and style loss function applied in the LossHandler\n",
    "    def gram_matrix(self, input_tensor):\n",
    "        \"\"\"raw gram matrix calculation\"\"\"\n",
    "        return tf.linalg.einsum('bijkc,bijkd->bcd', input_tensor, input_tensor)\n",
    "    \n",
    "    def gram_matrix_gatys(self, input_tensor):\n",
    "        \"\"\"original gram calc by gatys, k added for 3rd dimension\"\"\"\n",
    "        result = self.gram_matrix(input_tensor)\n",
    "        input_shape = tf.shape(input_tensor)\n",
    "        num_locations = tf.cast(input_shape[1]*input_shape[2]*input_shape[3], tf.float32)\n",
    "        return result/(num_locations)\n",
    "\n",
    "    def gram_matrix_hri_standardized(self, input_tensor):\n",
    "        \"\"\"hri standardized gram calc\"\"\"\n",
    "        result = self.gram_matrix(input_tensor)\n",
    "        result = (result - tf.math.reduce_mean(result)) / tf.math.reduce_std(result)\n",
    "        return result * 1000\n",
    "\n",
    "    def style_loss_gatys(self, responses): \n",
    "        \"\"\"style loss gatys\"\"\"\n",
    "        grams = [self.style_gram_fcn(r) for r in responses]\n",
    "        style_loss = tf.add_n([slw * tf.reduce_mean((g1-g2)**2) for g1, g2, slw in zip(grams, self.style_grams, self.style_layer_weights)])\n",
    "        style_loss *= self.style_weight / self.num_style_layers\n",
    "        return style_loss\n",
    "\n",
    "    def content_loss_gatys(self, response):\n",
    "        \"\"\"content loss gatys\"\"\"\n",
    "        content_loss = tf.add_n([tf.reduce_mean((r1-r2)**2) for r1, r2 in zip(response, self.content_responses)])\n",
    "        content_loss *= self.content_weight / self.num_content_layers\n",
    "        return content_loss      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "Jt3i3RRrJiOX"
   },
   "outputs": [],
   "source": [
    "# regularization related functions applied in the LossHandler  \n",
    "def clip_0_1(voxel):\n",
    "    return tf.clip_by_value(voxel, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "\n",
    "def high_pass_x_y_z(voxel):\n",
    "    z_var = voxel[:,:,:,1:,:] - voxel[:,:,:,:-1,:]\n",
    "    y_var = voxel[:,:,1:,:,:] - voxel[:,:,:-1,:,:]\n",
    "    x_var = voxel[:,1:,:,:,:] - voxel[:,:-1,:,:,:]\n",
    "    return x_var, y_var, z_var\n",
    "\n",
    "\n",
    "def total_variation_loss(voxel):\n",
    "    x_deltas, y_deltas, z_deltas = high_pass_x_y_z(voxel)\n",
    "    return tf.reduce_mean(x_deltas**2) + tf.reduce_mean(y_deltas**2) + tf.reduce_mean(z_deltas**2)\n",
    "\n",
    "\n",
    "def symmetry_loss(v):\n",
    "    \"\"\"Used for symmetry enforcement of ModelNEt 40 cars, sym along y axis\"\"\"\n",
    "    vsize = 256 #v.shape[2]\n",
    "    vh1 = tf.slice(v, [0,0,0,0,0], [1,vsize,vsize//2,vsize,1]) # split in y axis, left/right ModelNet40 cars\n",
    "    vh2 = tf.slice(v, [0,0,vsize//2,0,0], [1,vsize,vsize//2,vsize,1])\n",
    "    vh2 = tf.reverse(vh2, axis=[2])\n",
    "    loss_symreg = tf.reduce_mean(tf.math.square(vh1 - vh2))\n",
    "    return loss_symreg\n",
    "\n",
    "\n",
    "def get_distance_matrix(v, tree_type='kd', padding=16):\n",
    "    # returns a 3D matrix containing distance values to the voxel-shape-surface with a neutral padding zone around the shape\n",
    "    \n",
    "    # Create point set for surface voxels and all zero voxels outside the model\n",
    "    v_surf = np.copy(v)\n",
    "    v_temp = ndimage.minimum_filter(v, size=3)\n",
    "    v_surf = v - v_temp\n",
    "    psurf = np.array(np.nonzero(v_surf)).transpose()\n",
    "    v_void = -(v_surf-1)\n",
    "    pvoid = np.array(np.nonzero(v_void)).transpose()\n",
    "\n",
    "    #sklearn trees\n",
    "    if tree_type == 'kd':\n",
    "        tree = KDTree(psurf)\n",
    "    elif tree_type == 'ball':\n",
    "        tree = BallTree(psurf)\n",
    "    else:\n",
    "        raise ValueError('Unknown value for tree_type') \n",
    "    \n",
    "    pvoid_dist, _ = tree.query(pvoid, k=1)\n",
    "    pvoid_dist = pvoid_dist.squeeze()\n",
    "    \n",
    "    # pcl -> matrix\n",
    "    pvoid_dist_matrix = np.zeros(v.shape)\n",
    "    pvoid_dist_matrix[tuple(pvoid.T)] = pvoid_dist\n",
    "    \n",
    "    pvoid_dist_matrix -= padding\n",
    "    pvoid_dist_matrix[pvoid_dist_matrix < 0] = 0   \n",
    "    return pvoid_dist_matrix\n",
    "\n",
    "\n",
    "def get_borreg_matrix(shape, margin):\n",
    "    # return a 3D matrix with penalty values on the border region\n",
    "    b = margin\n",
    "    borreg_matrix = np.zeros(shape).astype(np.float32)\n",
    "    borreg_matrix[0,0:b,:,:,0] = 1\n",
    "    borreg_matrix[0,:,0:b,:,0] = 1\n",
    "    borreg_matrix[0,:,:,0:b,0] = 1\n",
    "    borreg_matrix[0,-b:,:,:,0] = 1\n",
    "    borreg_matrix[0,:,-b:,:,0] = 1\n",
    "    borreg_matrix[0,:,:,-b:,0] = 1\n",
    "    return tf.constant(borreg_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment LOOP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'myExperiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each value has to be wrapped in a list ! even if single value\n",
    "loop_conf = dict()\n",
    "loop_conf['seed'] = [13]\n",
    "loop_conf['save_modulo'] = [9999999999]\n",
    "\n",
    "loop_conf['style_loss_type'] = ['hri_std'] #hri_std, gatys\n",
    "loop_conf['content_loss_type'] = ['gatys']\n",
    "\n",
    "loop_conf['content_voxel'] = ['bunny224'] \n",
    "loop_conf['style_voxel'] = [ 'cube131'] \n",
    "loop_conf['initial_voxel'] = ['bunny224'] \n",
    "\n",
    "loop_conf['content_layer_names'] = [\n",
    "#     ['tf_op_layer_concat'],\n",
    "#     ['tf_op_layer_concat_1'],\n",
    "#     ['tf_op_layer_concat_2'],\n",
    "#     ['tf_op_layer_concat_3'],\n",
    "    ['tf_op_layer_concat_4'],\n",
    "#     ['tf_op_layer_concat_5'],\n",
    "#     ['tf_op_layer_concat_2'],\n",
    "#     ['tf_op_layer_concat_1'],\n",
    "#     ['tf_op_layer_concat_1'],\n",
    "]\n",
    "\n",
    "loop_conf['style_layer_names'] = [\n",
    "#     ['tf_op_layer_concat'],\n",
    "#     ['tf_op_layer_concat', 'tf_op_layer_concat_1'],\n",
    "#     ['tf_op_layer_concat', 'tf_op_layer_concat_1', 'tf_op_layer_concat_2'],\n",
    "#     ['tf_op_layer_concat', 'tf_op_layer_concat_1', 'tf_op_layer_concat_2', 'tf_op_layer_concat_3'],\n",
    "    ['tf_op_layer_concat', 'tf_op_layer_concat_1', 'tf_op_layer_concat_2', 'tf_op_layer_concat_3', 'tf_op_layer_concat_4'],\n",
    "#     ['tf_op_layer_concat', 'tf_op_layer_concat_1', 'tf_op_layer_concat_2', 'tf_op_layer_concat_3', 'tf_op_layer_concat_4', 'tf_op_layer_concat_5'],\n",
    "#     ['tf_op_layer_concat_1', 'tf_op_layer_concat_2', 'tf_op_layer_concat_3', 'tf_op_layer_concat_4'],\n",
    "]\n",
    "loop_conf['style_layer_weights'] = [ # keep it sum = num(layers)\n",
    "    [1,1,1,1,1,1,1,1],\n",
    "#     [2,1,0.5,0.5],\n",
    "#     [0.5,0.5,1,2],\n",
    "]\n",
    "\n",
    "loop_conf['optimizer'] = ['adam']\n",
    "loop_conf['lr'] = [0.01]\n",
    "loop_conf['epochs'] = [2]#[40]\n",
    "loop_conf['steps_per_epoch'] = [2]#[500]\n",
    "loop_conf['style_weight'] = [20]\n",
    "loop_conf['content_weight'] = [4]\n",
    "loop_conf['total_variation_weight'] = [0] \n",
    "loop_conf['dist_reg_weight'] = [0]#[500] \n",
    "loop_conf['dist_reg_padding'] = [15]\n",
    "loop_conf['sym_reg_weight'] = [0] #[1e7]\n",
    "loop_conf['bor_reg_weight'] = [10]\n",
    "loop_conf['bor_reg_margin'] = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# loop_conf sanity check\n",
    "for lk, lv in loop_conf.items():\n",
    "    assert type(lv) == list, f\"{lk} entry not a list\"\n",
    "l = [len(l) for l in loop_conf.values()]\n",
    "l = set(l) \n",
    "ltest = l - set([1])\n",
    "assert len(ltest) <= 1 , f\"Misconfiguration of loop_conf. Multiple non 1 values found: {l}\"\n",
    "\n",
    "# transform into list of dicts for the loop\n",
    "lc = []\n",
    "for i in range(max(l)):\n",
    "    lc.append(dict())\n",
    "    for k in loop_conf.keys():\n",
    "        lc[i][k] = loop_conf[k][min(i, len(loop_conf[k])-1 )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_global = dict()\n",
    "\n",
    "\n",
    "# Documentation folder\n",
    "# Folder where the voxel models are stored as pickle files.\n",
    "# A copy of the executed NB is saved as an html file.\n",
    "conf_global['doc_dir_base'] = './results'\n",
    "\n",
    "\n",
    "# conf_global['vnet_file'] = f'{LOG_DIR}/model.38-0.48.hdf5'\n",
    "conf_global['vnet_file'] = f'./data/Net_belu.hdf5'\n",
    "conf_global['now'] = datetime.datetime.now()\n",
    "conf_global['experiment'] = EXPERIMENT_NAME\n",
    "conf_global['doc_dir'] = f\"{conf_global['doc_dir_base']}/{conf_global['now']:%Y%m%d_%H%M%S}_{conf_global['experiment']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Output directory:\")\n",
    "print(conf_global['doc_dir'])\n",
    "print()\n",
    "\n",
    "# create folder for documentation\n",
    "os.makedirs(conf_global['doc_dir'], exist_ok=True)\n",
    "assert(os.path.exists(conf_global['doc_dir']))\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# The configurations loop\n",
    "for i,c in enumerate(lc):\n",
    "    print('#'*10)\n",
    "    print(f'Start Loop: {i}')\n",
    "    \n",
    "    # clear TF\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # set seeds\n",
    "    tf.random.set_seed(c['seed'])\n",
    "    np.random.seed(c['seed'])\n",
    "\n",
    "    # Optimizer\n",
    "    if c['optimizer'] == 'adam':\n",
    "        opt = tf.optimizers.Adam(learning_rate=c['lr'], beta_1=0.99, epsilon=1e-1)\n",
    "    elif c['optimizer'] == 'sgd':\n",
    "        opt = tf.optimizers.SGD(learning_rate=c['lr'])\n",
    "    else:\n",
    "        raise ValueError('Unknown value for optimizer')\n",
    "\n",
    "    # Voxel shapes\n",
    "    i_voxel = tf.Variable(get_voxel(c['initial_voxel']))\n",
    "    s_voxel = tf.constant(get_voxel(c['style_voxel']))\n",
    "    c_voxel = tf.constant(get_voxel(c['content_voxel']))  \n",
    "    \n",
    "    # storing voxel shapes\n",
    "    pickle.dump( {\"voxel\": np.squeeze(i_voxel.numpy())} , open( os.path.join(conf_global['doc_dir'], f\"initial_{i}.pkl\" ), \"wb\" ) )\n",
    "    pickle.dump( {\"voxel\": np.squeeze(c_voxel.numpy())} , open( os.path.join(conf_global['doc_dir'], f\"content_{i}.pkl\" ), \"wb\" ) )\n",
    "    pickle.dump( {\"voxel\": np.squeeze(s_voxel.numpy())} , open( os.path.join(conf_global['doc_dir'], f\"style_{i}.pkl\" ), \"wb\" ) )\n",
    "\n",
    "    # get activation map extractor (includes neural networks)\n",
    "    response_extractor = ResponseExtractor(conf_global['vnet_file'], c['style_layer_names'], c['content_layer_names'])\n",
    "    \n",
    "    # loss functions\n",
    "    loss_handler = LossHandler(response_extractor,\n",
    "                               s_voxel,\n",
    "                               c_voxel,\n",
    "                               content_loss_type = c['content_loss_type'],\n",
    "                               style_loss_type = c['style_loss_type'],\n",
    "                               num_content_layers = len(c['content_layer_names']),\n",
    "                               num_style_layers = len(c['style_layer_names']),\n",
    "                               style_weight = c['style_weight'],\n",
    "                               style_layer_weights=c['style_layer_weights'],\n",
    "                               content_weight = c['content_weight'],\n",
    "                               total_variation_weight = c['total_variation_weight'],\n",
    "                               dist_reg_weight = c['dist_reg_weight'],\n",
    "                               dist_reg_padding = c['dist_reg_padding'],\n",
    "                               sym_reg_weight = c['sym_reg_weight'],\n",
    "                               bor_reg_weight = c['bor_reg_weight'],\n",
    "                               bor_reg_margin = c['bor_reg_margin'],\n",
    "                              )\n",
    "    \n",
    "    # training function\n",
    "    @tf.function()\n",
    "    def train_step(voxel, debug=False):\n",
    "        with tf.GradientTape() as tape:\n",
    "            losses = loss_handler(voxel)\n",
    "        grad = tape.gradient(losses['total'], voxel)\n",
    "        opt.apply_gradients([(grad, voxel)])\n",
    "        voxel.assign(clip_0_1(voxel))\n",
    "        return losses\n",
    "\n",
    "\n",
    "    # the optimization\n",
    "    voxel = i_voxel\n",
    "    losses = train_step(voxel, True)\n",
    "    \n",
    "    # loss history and printing\n",
    "    losshist = dict()\n",
    "    for k in losses.keys():\n",
    "        losshist[k] = [losses[k].numpy()]\n",
    "    print(\"Initial losses:  \", end=\"\")\n",
    "    for k,v in losshist.items():\n",
    "        print(f\"{k}: {v[-1]:10.1f}\", end=\"  \")\n",
    "    print()\n",
    "        \n",
    "    # optimization loop    \n",
    "    step = 0\n",
    "    for n in range(c['epochs']):\n",
    "        for m in range(c['steps_per_epoch']-1):\n",
    "            step += 1\n",
    "            train_step(voxel)\n",
    "        print(f\"Epoch {n}:  \", end=\"\")\n",
    "        \n",
    "        for k in losshist.keys():\n",
    "            losshist[k].append(train_step(voxel, True)[k].numpy())\n",
    "        for k,v in losshist.items():\n",
    "            print(f\"{k}: {v[-1]:10.1f}\", end=\"  \")\n",
    "        print()\n",
    "        \n",
    "        if n % c['save_modulo'] == 0 and n > 0:\n",
    "            pickle.dump( {\"voxel\": voxel.numpy(), \"losshist\": losshist} , open( os.path.join(conf_global['doc_dir'], f\"result_{i}_{n}.pkl\" ), \"wb\" ) )\n",
    "\n",
    "    print(f\"Finished  Loop: {i}\")\n",
    "\n",
    "    # Final documentation\n",
    "    pickle.dump( {\"voxel\": voxel.numpy(), \"losshist\": losshist} , open( os.path.join(conf_global['doc_dir'], f\"result_{i}.pkl\" ), \"wb\" ) )\n",
    "    print(\"Optimization finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "The generated shapes and the initial shapes are stored in the ./results folder.\\\n",
    "The shapes are stored as pickle files as numpy matrices.\\\n",
    "The result.pkl also includes the optimization history.\\\n",
    "\n",
    "The result shape is non binary, meaning material is stored as a float between 0 and 1. For plotting, you have to set a materialno materiak threshold.\n",
    "\n",
    "We recommend ipyvolume for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Neural style transfer",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
